# 트러블슈팅 시나리오 면접 질문 & 답변

## 사용 방법
1. 질문을 먼저 읽고 스스로 답변해보세요
2. 답변을 확인하고 부족한 부분을 학습하세요
3. ⭐ 표시는 빈출 질문입니다

---

## Q1. 서버 응답이 느려졌을 때 어떻게 진단하나요? ⭐⭐⭐

<details>
<summary>답변 보기</summary>

### 진단 순서

```
┌─────────────────────────────────────────────────────┐
│ 1. 현상 파악                                         │
│    - 어떤 요청이 느린지? (전체? 특정 API?)           │
│    - 언제부터? 갑자기? 점진적?                       │
│    - 영향 범위는?                                    │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 2. 시스템 리소스 확인                                │
│    - CPU: top, htop                                  │
│    - Memory: free -h                                 │
│    - Disk: df -h, iostat                            │
│    - Network: netstat, ss                           │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 3. 애플리케이션 레벨                                 │
│    - 로그 확인 (에러, 슬로우 로그)                   │
│    - 연결 풀 상태 (DB, Redis)                        │
│    - 스레드/프로세스 수                              │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 4. 외부 의존성                                       │
│    - DB 쿼리 성능                                    │
│    - 외부 API 응답 시간                              │
│    - 캐시 히트율                                     │
└─────────────────────────────────────────────────────┘
```

### 실제 진단 명령어

```bash
# 1. 시스템 전반적 상태
top
vmstat 1 5

# 2. CPU 사용 프로세스
ps aux --sort=-%cpu | head

# 3. 메모리 사용
free -h
ps aux --sort=-%mem | head

# 4. 디스크 I/O
iostat -x 1 5
iotop

# 5. 네트워크 연결
ss -s
ss -ant | awk '{print $1}' | sort | uniq -c

# 6. 애플리케이션 로그
tail -f /var/log/app/error.log
grep -i "slow\|timeout\|error" /var/log/app/*.log
```

### 면접 답변 예시

```
"프로덕션에서 API 응답 시간이 평소 200ms에서 2초로 증가한 적이 있습니다.

1. 먼저 top으로 확인하니 CPU는 정상이었지만
   메모리 사용률이 90%를 넘었습니다.

2. ps로 확인하니 특정 Java 프로세스가 메모리를 많이 점유했고
   GC 로그를 보니 Full GC가 빈번하게 발생하고 있었습니다.

3. heap dump를 떠서 분석하니 특정 API에서
   대용량 리스트를 메모리에 한번에 로드하고 있었습니다.

4. 해당 로직을 페이지네이션으로 수정하고
   힙 사이즈도 조정하여 해결했습니다.

5. 이후 메모리 사용률 80% 알림을 추가하고
   APM 도구로 상시 모니터링을 강화했습니다."
```

</details>

---

## Q2. DB 커넥션 풀 고갈 문제를 어떻게 해결했나요? ⭐⭐⭐

<details>
<summary>답변 보기</summary>

### 상황

```
증상:
- 애플리케이션 에러: "Cannot acquire connection from pool"
- 모든 API 요청 실패
- DB 자체는 정상
```

### 진단 과정

```bash
# 1. 현재 DB 연결 수 확인 (MySQL)
SHOW PROCESSLIST;
SELECT COUNT(*) FROM information_schema.processlist;

# 2. 연결 상태별 분포
SELECT command, COUNT(*)
FROM information_schema.processlist
GROUP BY command;

# 3. 슬로우 쿼리 확인
SHOW VARIABLES LIKE 'slow_query_log%';
# slow_query_log 파일 분석

# 4. 애플리케이션 커넥션 풀 모니터링
# HikariCP metrics 확인
```

### 주요 원인과 해결

```
원인 1: 슬로우 쿼리
┌─────────────────────────────────────────┐
│ 문제: 특정 쿼리가 30초 이상 소요         │
│ 해결: 인덱스 추가, 쿼리 최적화           │
└─────────────────────────────────────────┘

원인 2: 커넥션 누수
┌─────────────────────────────────────────┐
│ 문제: 커넥션 반환 안됨 (try-with 누락)   │
│ 해결: 코드 수정, leak detection 활성화   │
└─────────────────────────────────────────┘

원인 3: 풀 사이즈 부족
┌─────────────────────────────────────────┐
│ 문제: 트래픽 대비 풀 사이즈 작음         │
│ 해결: 풀 사이즈 조정, 타임아웃 설정      │
└─────────────────────────────────────────┘

원인 4: 트랜잭션 장시간 유지
┌─────────────────────────────────────────┐
│ 문제: @Transactional 범위 너무 큼        │
│ 해결: 트랜잭션 범위 최소화               │
└─────────────────────────────────────────┘
```

### 설정 예시 (HikariCP)

```yaml
spring:
  datasource:
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 3000
      idle-timeout: 600000
      max-lifetime: 1800000
      leak-detection-threshold: 60000
```

### 면접 답변 예시

```
"결제 API에서 간헐적으로 타임아웃이 발생했습니다.

진단:
1. 에러 로그에서 'Connection pool exhausted' 확인
2. DB processlist 확인 → 대부분 Sleep 상태로 대기 중
3. 슬로우 쿼리 로그 → 특정 조회 쿼리 20초 소요

원인:
결제 내역 조회 시 1년치 데이터를 풀스캔하고 있었음

해결:
1. 날짜 컬럼에 인덱스 추가 (20초 → 0.1초)
2. HikariCP leak detection 활성화
3. 커넥션 풀 모니터링 대시보드 추가

결과:
커넥션 고갈 재발 없음, 조회 성능 200배 개선"
```

</details>

---

## Q3. 메모리 누수(Memory Leak)를 어떻게 해결했나요? ⭐⭐

<details>
<summary>답변 보기</summary>

### 증상

```
- 시간이 지날수록 메모리 사용량 증가
- 주기적인 OOMKilled (Kubernetes)
- GC 시간 증가, 응답 지연
```

### 진단 과정 (Java)

```bash
# 1. 힙 사용량 확인
jstat -gc <pid> 1000

# 2. 힙 덤프 생성
jmap -dump:live,format=b,file=heap.hprof <pid>

# 3. 힙 덤프 분석 (MAT, VisualVM)
# - Dominator Tree: 큰 객체 확인
# - Leak Suspects: 누수 의심 객체

# 4. GC 로그 분석
-XX:+PrintGCDetails -XX:+PrintGCTimeStamps
```

### 흔한 원인

```
1. 캐시 무한 증가
┌─────────────────────────────────────────┐
│ HashMap에 데이터 추가만, 삭제 없음       │
│ 해결: LRU 캐시, TTL 설정                 │
└─────────────────────────────────────────┘

2. 이벤트 리스너 미해제
┌─────────────────────────────────────────┐
│ 등록 후 해제 안함                        │
│ 해결: removeListener, WeakReference     │
└─────────────────────────────────────────┘

3. 스레드 로컬 미정리
┌─────────────────────────────────────────┐
│ ThreadLocal 사용 후 remove() 안함       │
│ 해결: finally 블록에서 remove()         │
└─────────────────────────────────────────┘

4. 커넥션 미반환
┌─────────────────────────────────────────┐
│ DB/HTTP 커넥션 close() 누락            │
│ 해결: try-with-resources 사용          │
└─────────────────────────────────────────┘
```

### 면접 답변 예시

```
"서비스가 배포 후 3일이 지나면 느려지는 현상이 있었습니다.

진단:
1. Kubernetes에서 Pod가 OOMKilled로 재시작되는 것 확인
2. 메모리 그래프가 톱니바퀴 패턴 (계단식 증가)
3. heap dump 분석 → 사용자 세션 객체가 수백만 개

원인:
Redis 세션 저장 로직에서 예외 발생 시
로컬 HashMap에 백업 저장하는데, 삭제 로직이 없었음

해결:
1. HashMap 대신 Caffeine Cache (maxSize, TTL 설정)
2. Redis 연결 실패 시 재시도 로직 추가
3. 메모리 사용량 80% 알림 설정

결과:
메모리 안정적 유지, OOMKilled 재발 없음"
```

</details>

---

## Q4. 배포 후 장애가 발생하면 어떻게 대응하나요? ⭐⭐⭐

<details>
<summary>답변 보기</summary>

### 대응 프로세스

```
┌─────────────────────────────────────────────────────┐
│ 1. 롤백 먼저! (원인 분석은 나중)                     │
│    "서비스 복구가 최우선"                            │
└──────────────────────┬──────────────────────────────┘
                       │ 5분 이내
                       ▼
┌─────────────────────────────────────────────────────┐
│ 2. 상황 전파                                         │
│    - Slack 장애 채널 알림                            │
│    - 관련자 소집                                     │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 3. 롤백 실행                                         │
│    - kubectl rollout undo                           │
│    - 이전 이미지 태그로 재배포                       │
└──────────────────────┬──────────────────────────────┘
                       │ 복구 확인
                       ▼
┌─────────────────────────────────────────────────────┐
│ 4. 원인 분석 (사후)                                  │
│    - 로그/메트릭 분석                                │
│    - 변경사항 리뷰                                   │
│    - 재현 테스트                                     │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 5. Postmortem 작성                                   │
│    - 타임라인 정리                                   │
│    - 재발 방지 Action Items                         │
└─────────────────────────────────────────────────────┘
```

### 롤백 명령어

```bash
# Kubernetes
kubectl rollout undo deployment/myapp
kubectl rollout status deployment/myapp

# Docker Compose
docker-compose up -d --no-deps myapp:previous-tag

# ArgoCD
# Git에서 이전 커밋으로 revert → 자동 동기화

# AWS ECS
aws ecs update-service --cluster prod --service myapp \
  --task-definition myapp:previous-revision
```

### 배포 전 체크리스트

```
☐ 스테이징 환경 테스트 완료
☐ 카나리/블루그린 배포 설정
☐ 롤백 계획 준비
☐ 모니터링 대시보드 열어두기
☐ 온콜 담당자 인지
☐ 배포 시간 (피크 시간 피하기)
```

### 면접 답변 예시

```
"신규 기능 배포 직후 에러율이 30%로 급증한 적이 있습니다.

대응:
1. (0분) 알림 확인 즉시 Slack에 장애 선언
2. (2분) kubectl rollout undo로 즉시 롤백
3. (5분) 에러율 정상화 확인

원인 분석:
- 새 버전에서 외부 API 엔드포인트가 변경됨
- 환경변수는 구버전 값 그대로

재발 방지:
1. 환경변수 변경 시 PR 리뷰 필수
2. 스테이징에서 외부 연동 테스트 추가
3. Canary 배포로 5%부터 점진적 확대

결과:
MTTR 5분 → 업무 시간 서비스 영향 최소화"
```

</details>

---

## Q5. 대규모 트래픽을 어떻게 처리했나요? ⭐⭐

<details>
<summary>답변 보기</summary>

### 대응 전략

```
┌─────────────────────────────────────────────────────┐
│              트래픽 급증 대응 전략                   │
├─────────────────────────────────────────────────────┤
│                                                      │
│  1. Scale Out (수평 확장)                            │
│     └── Auto Scaling, HPA                           │
│                                                      │
│  2. 캐싱                                             │
│     └── CDN, Redis, 애플리케이션 캐시               │
│                                                      │
│  3. 큐잉 (비동기 처리)                               │
│     └── Kafka, SQS, 요청 버퍼링                     │
│                                                      │
│  4. 서킷 브레이커                                    │
│     └── 장애 전파 차단                               │
│                                                      │
│  5. Rate Limiting                                    │
│     └── API 호출 제한                                │
│                                                      │
└─────────────────────────────────────────────────────┘
```

### Auto Scaling 설정

```yaml
# Kubernetes HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### 캐싱 전략

```
┌──────────────┬──────────────┬───────────────────┐
│    레벨      │    도구      │      용도          │
├──────────────┼──────────────┼───────────────────┤
│ CDN          │ CloudFront   │ 정적 자원          │
│ API Gateway  │ 캐시 정책    │ API 응답 캐시      │
│ Application  │ Caffeine     │ 로컬 캐시          │
│ Distributed  │ Redis        │ 세션, 핫데이터     │
│ Database     │ Query Cache  │ 쿼리 결과          │
└──────────────┴──────────────┴───────────────────┘
```

### 면접 답변 예시

```
"이벤트 프로모션 때 평소 대비 10배 트래픽이 예상되었습니다.

준비:
1. 부하 테스트로 병목 구간 파악
   - API 서버: CPU 80%에서 응답 지연
   - DB: 커넥션 풀 고갈

2. 대응 조치
   - HPA 설정 (max 20 → 50)
   - Redis 캐시 적용 (상품 목록, TTL 1분)
   - DB Read Replica 추가
   - CDN으로 이미지 서빙

3. 이벤트 당일 모니터링
   - 실시간 대시보드 상시 확인
   - 슬랙 알림 즉시 대응 체계

결과:
- 피크 시 Pod 35개까지 확장
- 평균 응답시간 150ms 유지
- 장애 없이 이벤트 종료"
```

</details>

---

## Q6. 보안 인시던트 대응 경험이 있나요? ⭐⭐

<details>
<summary>답변 보기</summary>

### 일반적인 보안 인시던트

```
1. 무차별 대입 공격 (Brute Force)
2. SQL Injection 시도
3. 자격증명 유출
4. DDoS 공격
5. 취약점 악용 시도
```

### 대응 프로세스

```
┌─────────────────────────────────────────────────────┐
│ 1. 탐지 (Detection)                                 │
│    - WAF 알림                                        │
│    - 비정상 로그인 시도 감지                         │
│    - 트래픽 이상 탐지                                │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 2. 격리 (Containment)                               │
│    - 의심 IP 차단                                    │
│    - 계정 잠금                                       │
│    - 영향 범위 파악                                  │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 3. 제거 (Eradication)                               │
│    - 취약점 패치                                     │
│    - 악성코드 제거                                   │
│    - 자격증명 재발급                                 │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 4. 복구 (Recovery)                                  │
│    - 서비스 정상화                                   │
│    - 모니터링 강화                                   │
└──────────────────────┬──────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────┐
│ 5. 사후 조치 (Lessons Learned)                      │
│    - 인시던트 보고서                                 │
│    - 보안 강화 조치                                  │
└─────────────────────────────────────────────────────┘
```

### 면접 답변 예시

```
"로그인 API에서 특정 IP 대역의 비정상적인 요청이 탐지되었습니다.

탐지:
- WAF에서 분당 1000회 이상 로그인 시도 알림
- 실패율 99% 이상 (무차별 대입 공격)

대응:
1. (즉시) 해당 IP 대역 WAF에서 차단
2. (10분) 영향받은 계정 분석 → 침해 없음 확인
3. (1시간) Rate Limiting 강화 (분당 10회)
4. (당일) Captcha 도입 (5회 실패 시)

사후 조치:
- 로그인 실패 모니터링 대시보드 추가
- 비정상 로그인 시도 시 사용자 알림 기능
- 정기 보안 점검 체계 수립

결과:
동일 유형 공격 자동 차단, 사용자 계정 안전 확보"
```

</details>

---

## 학습 체크리스트

- [ ] 성능 저하 시 진단 순서를 설명할 수 있다
- [ ] DB 커넥션 풀 이슈 해결 경험을 말할 수 있다
- [ ] 메모리 누수 진단/해결 방법을 안다
- [ ] 배포 후 롤백 프로세스를 설명할 수 있다
- [ ] 트래픽 급증 대응 전략을 말할 수 있다
- [ ] 보안 인시던트 대응 경험을 말할 수 있다
